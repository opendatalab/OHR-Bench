{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"../data/qas_v2.json\") as f:\n",
    "    qa_dict = {item[\"ID\"]: item for item in json.load(f)}\n",
    "\n",
    "paths = glob(\"../output/**/*.json\", recursive=True)\n",
    "ocr_types = [\"gt\"]\n",
    "ret_df = []\n",
    "gen_df = []\n",
    "end_df = []\n",
    "for path in tqdm(paths):\n",
    "    if \"gpt-4o\" in path or \"qwen2_72b\" in path:\n",
    "        continue\n",
    "    basename = os.path.basename(path).removesuffix(\".json\")\n",
    "    ocr_type = os.path.basename(os.path.dirname(path))\n",
    "    if ocr_type not in ocr_types:\n",
    "        continue\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    if \"/retrieval/\" in path:\n",
    "        ret = basename.split(\"_\")[1]\n",
    "        llm = \"\"\n",
    "        df = ret_df\n",
    "    elif \"/generation/\" in path:\n",
    "        ret = \"\"\n",
    "        llm = \"_\".join(basename.split(\"_\")[-2:])\n",
    "        df = gen_df\n",
    "    elif \"/end2end/\" in path:\n",
    "        ret = basename.split(\"_\")[1]\n",
    "        llm = \"_\".join(basename.split(\"_\")[-2:])\n",
    "        df = end_df\n",
    "    df.extend({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"ocr_type\": ocr_type,\n",
    "        \"ret\": ret,\n",
    "        \"llm\": llm,\n",
    "        \"domain\": qa_dict[item[\"id\"]][\"doc_type\"],\n",
    "        \"doc_name\": qa_dict[item[\"id\"]][\"doc_name\"].split(\"/\")[-1],\n",
    "        \"evidence_source\": qa_dict[item[\"id\"]][\"evidence_source\"],\n",
    "        \"answer_form\": qa_dict[item[\"id\"]][\"answer_form\"],\n",
    "        **item[\"metrics\"]\n",
    "    } for item in data[\"results\"] if item[\"id\"] in qa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_df = pd.DataFrame(end_df)\n",
    "gen_df = pd.DataFrame(gen_df)\n",
    "ret_df = pd.DataFrame(ret_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gen Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "keys = [\"ocr_type\", \"ret\", \"llm\", \"domain\", \"doc_name\", \"evidence_source\", \"answer_form\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def show_types(input_df, ocr_types, metric=\"F1\", domain=False,):\n",
    "    grouby = \"evidence_source\"\n",
    "    if domain:\n",
    "        grouby = \"domain\"\n",
    "    from copy import deepcopy\n",
    "    import pandas as pd\n",
    "    \n",
    "    input_df = deepcopy(input_df)\n",
    "\n",
    "    input_df[metric] = input_df[metric] * 100\n",
    "    \n",
    "    evidence_order = {'text': 0, 'table': 1, 'formula': 2, 'chart': 3, 'reading_order': 4, 'multi': 5, 'all': 6}\n",
    "    df_filtered = input_df[input_df[\"ocr_type\"].isin(ocr_types)]\n",
    "    result = (\n",
    "        df_filtered[keys + [metric]]\n",
    "        .groupby([\"ocr_type\", grouby])\n",
    "        .agg(\n",
    "            mean_metric=(metric, 'mean'),\n",
    "            count=(metric, 'count')\n",
    "        )\n",
    "    )\n",
    "    overall = (\n",
    "        df_filtered[keys + [metric]]\n",
    "        .groupby(\"ocr_type\")\n",
    "        .agg(\n",
    "            mean_metric=(metric, 'mean'),\n",
    "            count=(metric, 'count')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    overall[grouby] = \"all\"\n",
    "    overall = overall.set_index([\"ocr_type\", grouby])\n",
    "    \n",
    "    final_result = pd.concat([result, overall])\n",
    "    final_result = final_result.reset_index()\n",
    "    final_result['evidence_order_value'] = final_result[grouby].map(evidence_order)\n",
    "    final_result = final_result.sort_values(by=['ocr_type', 'evidence_order_value'])\n",
    "    final_result = final_result.drop(columns=['evidence_order_value']).set_index(['ocr_type', grouby])\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(show_types(ret_df, ocr_types, \"lcs\", domain=False).round(1).pivot_table(index='evidence_source', columns='ocr_type', values='mean_metric', aggfunc=lambda x: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(show_types(gen_df, ocr_types, domain=False).round(1).pivot_table(index='evidence_source', columns='ocr_type', values='mean_metric', aggfunc=lambda x: x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(show_types(end_df, ocr_types, domain=False).round(1).pivot_table(index='evidence_source', columns='ocr_type', values='mean_metric', aggfunc=lambda x: x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MinerU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
